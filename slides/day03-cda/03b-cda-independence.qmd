## $\chi^2$ Test of Independence

The $\chi^2$ test of independence is a statistical test used to determine whether there is a significant association between two categorical variables. 

::: vertical-space
:::

It helps to assess whether the occurrence of one categorical variable is related to the occurrence of another categorical variable


## How important is it to ask pointed questions? {.smaller}

The following is example is taken from [Chapter 18 of Introduction to Modern Statistics](https://openintro-ims.netlify.app/inference-tables.html)

::: vertical-space
:::

In this experiment, 219 individuals were asked to be a seller of an iPod
The participant received $10 + 5% of the sale price for participating. 
The iPod they were selling had frozen twice in the past inexplicably but otherwise worked fine.

::: vertical-space
:::

The prospective buyer starts off and then asks one of three final questions, depending on the seller's treatment group.

- **General:** What can you tell me about it ?
- **Positive Assumption:** It doesn't have any problems, does it ?
- **Negative Assumption:** What problem does it have ?



The outcome variable is whether or not the participant discloses or hides the problem with the iPod


:::footer
[*Source: Minson JA, Ruedy NE, Schweitzer ME. There is such a thing as a stupid question: Question disclosure in strategic communication*](https://www.acrwebsite.org/volumes/1012889/volumes/v40/NA-40)
:::

##

The following data is the `ask` dataset from the R package `openintro`

```{r}
#| echo: true
ask_two_way <- openintro::ask |> 
  select(question_class, response) |> 
  table() |> 
  addmargins()
```



```{r}
attr(ask_two_way,'dimnames')$question_class <- c('General','Negative_assumption','Positive_assumption','Total')
attr(ask_two_way,'dimnames')$response <- c('Disclose','Hide','Total')
knitr::kable(ask_two_way)
```

::: vertical-space
:::


To perform the $\chi^2$ test of independence, the data is organized in a contingency table *(shown above)*.
This table displays the frequencies of the different combinations of categories for the two variables



##

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show code"
#| fig-align: center
openintro::ask |> 
  ggplot() +
  aes(x = question_class, fill = response) +
  geom_bar(position = "fill") +
  scale_fill_manual(values=c('#8dc4b0','#948DC4')) +
  theme(axis.text.x = element_text(angle = 45,vjust = .5)) +
  labs(x = "Question Class",
       y = "Proportion")
```


If there were no relationship, we would expect to see the purple bars reaching to the same height, regardless of the question class





## {.smaller}

:::callout-note
## Expected Counts
To calculate the expected count for the $i^{th}$ row and $j^{th}$ column, compute
$$
\text{Expected Count}_{\,row \, i,\, \;col \, j } = 
\frac{
(\text{row}\,\; i \; \text{total}) \times (\text{column}\,\; j \; \text{total}) 
}{
\text{table total}
}
$$
:::



::: vertical-space
:::



:::: {.columns}

::: {.column width="50%"}
```{r}
knitr::kable(ask_two_way)
```

:::

::: {.column width="50%"}
$\text{Expected Count}_{\,row \, 1,\, \;col \, 1 } =$

$$
\frac{(\text{row 1 total}) \times (\text{column 1 total}) }{
(\text{table total})
} =
$$

::: {.fragment}

$$
\frac{73 \times 61}{219} = 20.33
$$
:::


::: vertical-space
:::


$\text{Expected Count}_{\,row \, 1,\, \;col \, 2 } =$

$$
\frac{(\text{row 1 total}) \times (\text{column 2 total}) }{
(\text{table total})
} =
$$

::: {.fragment}


$$
\frac{73 \times 158}{219} = 52.67
$$







::::

::: vertical-space
:::



:::
Compute the expected counts for the remaining cells

:::



##


```{r}
expected_counts <- tibble::tribble(
  ~`Question Type`, ~`Disclose (Expected)`, ~`Hide (Expected)`, ~Total,
  'General','2  (20.33)','71  (52.67)','73',
  'Positive assumption','23  (20.33)','50  (52.67)', '73',
  'Negative assumption','36  (20.33)','37   (52.67)', '73',
  'Total', '61', '158', '219'
)
```


```{r}
knitr::kable(expected_counts)
```


##


The $\chi^2$ test statistic for a two-way table is found by finding the ratio of how far the observed counts are from the expected counts


:::callout-note
## $\chi^2$ test-statistic
The test statistic for assessing the independence between two categorical variables

$$
\chi^2 = \sum_{ij} \frac{
(\text{observed count - expected count})^2
}{
\text{expected count}
}
$$
When the null hypothesis is true and the conditions are met, $\, \chi^2$ has a Chi-squared distribution with 
$\, df = (r-1) \times (c-1)$, where $r=$ number of rows, and $c=$ number of columns
:::

Conditions:

- Independent observations
- Large samples: At least 5 expected counts in each cell



## {.smaller}


:::: {.columns}

::: {.column width="50%"}

```{r}
expected_counts <- tibble::tribble(
  ~`Question Type`, ~`Disclose (Expected)`, ~`Hide (Expected)`, ~Total,
  'General','2  (20.33)','71  (52.67)','73',
  'Positive assumption','23  (20.33)','50  (52.67)', '73',
  'Negative assumption','36  (20.33)','37   (52.67)', '73',
  'Total', '61', '158', '219'
)
```


```{r}
knitr::kable(expected_counts)
```

:::

::: {.column width="50%"}


**row 1, column 1**
$$
\frac{(2-20.33)^2}{20.33} = 16.53
$$
**row 2, column 1**
$$
\begin{align*}
\frac{(23-20.33)^2}{20.33} &= 0.35 \\
&\vdots
\end{align*}
$$
**row 3, column 2**
$$
\frac{(37-52.67)^2}{52.67} = 4.66
$$



:::

::::

::: vertical-space
:::

For each cell use the general formula
$$
\frac{(\text{observed - expected})^2}{\text{expected}}
$$




##


```{r}
observed_chi2 <- tibble::tribble(
  ~`Question Type`, ~`Disclose`, ~`Hide`,
  'General', 16.53, 6.38,
  'Positive assumption', 0.35, 0.14,
  'Negative assumption',12.07, 4.66
)
```


```{r}
knitr::kable(observed_chi2)
```


:::vertical-space
:::

Adding the computed value for each cell gives the $\chi^2$ test statistic
$$
\chi^2 = 16.53 + 0.53 + \cdots + 4.66 = 40.13
$$


::: vertical-space
:::


Does the value of the $\chi^2=40.13$ test statistic indicate that the observed and expected values are really different ?

Or is $\chi^2 = 40.13$ a value we'd expect to see just due to natural variability


## {.smaller}

$\chi^2$ distribution for differing degrees of freedom

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Show code"
x_vals <- 0:20
df_vals <- seq(2,10,2)

df_dat <- map_df(df_vals, ~ tibble(
  l = paste(.),
  x = x_vals,
  y = dchisq(x_vals, .)
))

ggplot(df_dat, aes(x, y, color = factor(l, levels = df_vals))) +
  geom_line(data = df_dat, 
             aes(x, y, color = factor(l, levels = df_vals))) +
  labs(title = "Chi-square Distribution",
       subtitle = "various degrees of freedom",
       x = "",y = 'Density',color = "Degrees of Freedom")
```


The larger the degrees of freedom, the long the right tail extends. The smaller the degrees of freedom, the more peaked the mode on the left becomes

## {.smaller}

We need to understand the variability in the values of the $\chi^2$ statistic we would expect to see if the null hypothesis was true *(i.e, $\chi^2$ distribution with $df = (3-2) \times (2-1) = 2$)*


```{r}
library(infer)
observed_indep_statistic <- openintro::ask |> 
  specify(response ~ question_class) |> 
  hypothesize(null = "independence") |> 
  calculate(stat = "Chisq")

openintro::ask |> 
  specify(response ~ question_class) |> 
  assume(distribution = "Chisq") |> 
  visualize() + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")
```


The red vertical line is our observed $\chi^2=40.13$ test statistic


##


We can now compute the $\,p$-value and draw a conclusion about whether the question affects the sellers likelihood of reporting the iPod problems

```{r}
#| echo: true
1 - pchisq(40.13, df = 2)
```


::: vertical-space
:::

::: vertical-space
:::


Using a significance level of $\, \alpha = 0.05,$ the null hypothesis is rejected since the $\,p$-value is smaller



::: vertical-space
:::

That is, the data provide convincing evidence that the question asked did affect a sellerâ€™s likelihood to tell the truth about problems with the iPod.





## Test of Independence: Using `Infer`

We will conduct a $\chi^2$ test of independence to determine whether there is a relationship between the question class and the disclosure/hiding of the problem with the iPod by the participants.

::: {style="font-size = 70%;"}
The following implementation considers the use of the package `infer`, which is part of the `tidymodels` ecosystem
:::


```{r}
#| echo: true
#| eval: false
library(infer)
observed_indep_statistic <- openintro::ask |> 
  specify(response ~ question_class) |> 
  hypothesize(null = "independence") |> 
  calculate(stat = "Chisq")
```

```{r}
observed_indep_statistic
```


The observed $\chi^2$ statistic is 40.1.



##

Now we want to compare the observed $\chi^2 = 40.1$ statistic to a null 
distribution, generated under the assumption that the question class and the disclosure/hiding of the problem with the iPod are not actually related. 

::: vertical-space
:::

We generate the null distribution using a **randomization approach**


::: vertical-space
:::


The randomization approach approximates the null distribution by randomly shuffling the response and explanatory variables. This process ensures that each participant's question class is paired with a randomly chosen disclosure or hiding of the problem with the iPod from the sample. 


The purpose is to disrupt any existing association between the two variables



##

::: {.panel-tabset}

### Code

First, generate the null distribution using randomization

```{r}
#| echo: true
null_dist_sim <- openintro::ask |> 
  specify(response ~ question_class) |> 
  hypothesize(null = "independence") |> 
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "Chisq")
```


::: vertical-space
:::

To visualize both the randomization-based and theoretical null distribution, we can pipe the randomization-based null distribution into `visualize()`, and provide `method = "both"`

```{r}
#| echo: true
#| eval: false
null_dist_sim |> 
  visualize(method = "both") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")
```


### Output



```{r}
#| fig-width: 9
#| fig-height: 4
null_dist_sim |> 
  visualize(method = "both") + 
  shade_p_value(observed_indep_statistic,
                direction = "greater")
```


None of the 1,000 simulations had a $\chi^2$ value of at least 40.13. 
In fact, none of the simulated $\chi^2$ statistics came anywhere close to the observed statistic

:::


##


Using the simulated null distribution, we can approximate the $\,p\,$-value with `get_p_value`

```{r}
#| echo: true
null_dist_sim |> 
  get_p_value(obs_stat = observed_indep_statistic, direction = "greater")
```

::: vertical-space
:::

::: {style="font-size = 80%;"}
Therefore, if there were really no relationship between the question class and the disclosure/hiding of the problem with the iPod, our approximation of the probability that we would see a statistic as or more extreme than 40.1 is approximately 0
:::


##

Equivalently, the theory-based approach to calculate the $\,p\,$-value using the true $\chi^2$ distribution can be obtained using the base R function `pchisq`


```{r}
#| echo: true
pchisq(observed_indep_statistic$stat, 
       df = 2, lower.tail = FALSE)
```


::: vertical-space
:::


The `infer` package provides a wrapper function `chisq_test`, to carry out $\chi^2$ test of independence on `tidy data`

```{r}
#| echo: true
chisq_test(openintro::ask, response ~ question_class)
```

