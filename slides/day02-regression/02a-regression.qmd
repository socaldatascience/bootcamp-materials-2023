---
title: "Linear Regression"
author: "Dr. Jaynes"
format: 
  revealjs:
    slide-number: true
    logo: "https://socaldatascience.github.io/bootcamp-materials-2022/img/socalds-logo.png"
    theme: ["slide-style.scss"]
    incremental: false
---

## Overview (To be removed)
- 2 hours of lecture (1 hour each)
- 50 minutes of activity (2 25-mins each)
- The first activity will be done right at the beginning
- The second activity will be done through the lecture portion 
- 2 5-min breaks

## Data (To be removed)
- Penguin data
- Bayes Rules Data: Section 11.3

## Goals
- Review Linear Regression Concepts from your Data Science Courses 
- Conditions for Least Squares Regression and Diagnostics
- Multiple Linear Regression
- Model Evaluation 

## Linear Regression Activity
- Simple Linear Regression: 1) Numerical predictor, 2) categorical predictor
- Scatterplot
- Fit model in R
- Model output
- Model equation
- Interpretation of estimates
- Prediction
<!--
https://mdsr-book.github.io/mdsr2e/ch-foundations.html#sec:gooddesign
-->

## Linear Regression Review
- Scatterplot
- Response and explanatory variables
- Linear equation: Math vs stats class
- Using R to fit a linear model
- Equation of linear model
- Interpretation of estimates
- Extrapolation
- Prediction
- Residuals
- Least squares regression
<!--
https://r4ds.had.co.nz/model-basics.html#model-basics
-->

## Conditions for Least Squares Regression: LINE
<!--
https://openintro-ims.netlify.app/inf-model-slr.html#tech-cond-linmod
-->
- L: Linear model
- I: Independent observation
- N: Normality of residuals
- E: Equal/constant variability around the line for all values of the explanatory variable

## Linearity
- Probably the most important condition
- The data should have a linear trend
- If the data illustrate a non-linear trend, then more advanced regression methods could be considered

## Independent Observations
- Be aware of data that come from sequential observations in time as they come with an underlying structure that needs to be considered when modeling 
- Dependent data can bias results 
- For example, stock prices

## Nearly Normal Residuals
- Residuals should be nearly normal 
- This condition can often be influenced by outliers
- While important, this condition can often be avoided through considering bootstrap procedures

## Equal or Constant Variability
- The variability of points are the least squares line should remain roughly constant
- Data that do not satisfy this condition will potentially influence and mis-estimate the variability of the slope, impacting the inference

## When all technical conditions are met:
- The least squares regression model (and related inference)  has important extensions (which are not trivial to implement with bootstrapping and randomization tests). 
- In particular, random effects models, repeated measures, and interaction are all linear model extensions which require the above technical conditions.


## Multiple Linear Regression
- Numerical and categorical predictor (binary and multi-category); interaction between numerical and categorical
- Interpretation
- Prediction

## Model Evaluation
- R2
- RMSE
- Overfitting
- Splitting the data

## Evaluating Models: How do you know if your model is a good one?
<!--
https://mdsr-book.github.io/mdsr2e/ch-modeling.html#evaluating-models
-->
- Bias-variance trade-off
- Cross-validation
- Measuring prediction error for quantitative responses: RMSE, correlation, coefficienct of determination

## Bias-variance Trade-off
- Goals: models that minimize both bias and variance 
- However, these are essentially mutually exclusive goals 
- Complicated model: will have less bias, but will in general have higher variance
- Simple model: can reduce variance but at the cost of increased bias
- Optimal balance between bias and variance depends on the purpose for which the model is constructed (e.g., prediction vs. description of causal relationships) 

## Modeling Overfitting: Cross-validation

Data sets are often divided into two sets:

- Training: Set of data on which you build your model
- Testing: After your model is built, this set of data is used to test it by evaluating it against data that it has not previously seen.

"Perhaps the most elementary mistake in predictive analytics is to overfit your model to the training data, only to see it later perform miserably on the testing set."

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(openintro)
library(broom)
theme_set(theme_gray(base_size = 18))
```





<!--

## Data `babies` in `openintro` package

```{r echo = FALSE}


glimpse(babies)

```

##  Baby Weights
:::: {.columns}
::: {.column width="40%"}
```{r eval = FALSE, fig.align='center', message = FALSE, warning = FALSE}
ggplot(babies, 
       aes(x = gestation, y = bwt)) +
  geom_point()

```
:::

::: {.column width="60%"}
```{r echo = FALSE, fig.align='center', message = FALSE, fig.height= 6,warning = FALSE}


ggplot(babies, 
       aes(x = gestation, y = bwt)) +
  geom_point()

```
:::
::::

## Response and Explanatory Variables

| Variable    | Variable Name | Type of Variable |
|-------------|-----------------|---------|
| Response (y)    | Birth weight | Numeric |
| Explanatory (x) | Gestation           | Numeric |


## Insert Mine's UCI ICS 80 Sampling and Study Design (i.e. regression slides)

## Insert Mine's UCI STATS 115 Evaluating Regression Models
-->
